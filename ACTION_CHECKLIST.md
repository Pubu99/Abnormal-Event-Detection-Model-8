# ‚úÖ ACTION CHECKLIST - Training Your Model

**Generated**: October 13, 2025  
**Status**: Ready to execute  
**Estimated Time**: 20 hours total

---

## üéØ PRE-TRAINING CHECKLIST

### [ ] Step 1: Verify Environment (5 minutes)

```bash
cd /home/abnormal/Group34/Abnormal-Event-Detection-Model-8

# Check Python version
python --version  # Should be 3.9+

# Verify CUDA
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')"
```

**Expected Output**:
```
CUDA Available: True
GPU: NVIDIA GeForce RTX 5090
```

---

### [ ] Step 2: Run Setup Test (2 minutes)

```bash
python test_setup.py
```

**Expected**: All tests pass ‚úÖ

**If any test fails**:
- Missing packages? Run: `pip install -r requirements.txt`
- No CUDA? Check CUDA installation
- Config error? Check `configs/config.yaml` exists

---

### [ ] Step 3: Analyze Your Data (Optional, 3 minutes)

```bash
python analyze_data.py
```

**What to check**:
- ‚úÖ Train images: ~1,266,345
- ‚úÖ Test images: ~111,308
- ‚úÖ 14 classes present
- ‚úÖ Imbalance ratio: 4-5x (normal)

---

### [ ] Step 4: Configure Training (2 minutes)

**Edit `configs/config.yaml` if needed:**

```yaml
training:
  epochs: 50  # Options: 5 (test), 50 (fast), 100 (best)
  batch_size: 128  # Reduce to 64 if OOM
  
  # Speed optimizations (keep enabled!)
  compile_model:
    enabled: true  # ‚úÖ 30% faster
  mixed_precision: true  # ‚úÖ 2-3x faster
  
  # Generalization techniques (keep enabled!)
  sam:
    enabled: true  # ‚úÖ Better generalization
  swa:
    enabled: true  # ‚úÖ Better weights
  mixup_cutmix:
    enabled: true  # ‚úÖ Better augmentation
```

---

## üöÄ TRAINING EXECUTION

### [ ] Step 5: Start Training

**Choose your configuration:**

#### Option A: Quick Test (2 hours) - For Testing Pipeline
```bash
python train.py --epochs 5
```

#### Option B: Fast Training (18-20 hours) - **RECOMMENDED**
```bash
python train.py --epochs 50 --wandb
```

#### Option C: Best Accuracy (36-40 hours)
```bash
python train.py --epochs 100 --wandb
```

---

### [ ] Step 6: Monitor Training (While Running)

#### Terminal 1: Training
```bash
# Let this run...
python train.py --epochs 50 --wandb
```

#### Terminal 2: TensorBoard (Optional)
```bash
tensorboard --logdir outputs/logs/
# Open: http://localhost:6006
```

#### Browser: Weights & Biases (Recommended)
```bash
# First time only:
wandb login

# Then visit:
https://wandb.ai
```

**What to monitor**:
- ‚úÖ Loss decreasing
- ‚úÖ Validation accuracy increasing
- ‚úÖ No overfitting (train vs val gap <5%)
- ‚úÖ GPU utilization >90%

---

### [ ] Step 7: Check GPU Usage (Every Hour)

```bash
watch -n 1 nvidia-smi
```

**Expected**:
- GPU Utilization: 95-100%
- Memory Usage: 18-20 GB (out of 24 GB)
- Temperature: <85¬∞C

**If low GPU usage (<50%)**:
- Check data loading (might be bottleneck)
- Increase `num_workers` in config
- Verify `compile_model.enabled: true`

---

## üìä POST-TRAINING

### [ ] Step 8: Evaluate on Test Set

```bash
# Find your best model
ls outputs/logs/*/checkpoints/best_model.pth

# Run evaluation
python evaluate.py --checkpoint outputs/logs/<experiment_name>/checkpoints/best_model.pth
```

**Expected Results**:
- Test Accuracy: 93-95%
- F1-Score: 93-95%
- Generalization Gap: <2%

---

### [ ] Step 9: Analyze Results

**Files to check**:
- `outputs/results/evaluation.txt` - Metrics
- `outputs/results/confusion_matrix.png` - Per-class performance
- `outputs/logs/<experiment>/` - Training logs

**What to look for**:
- ‚úÖ High accuracy on all classes
- ‚úÖ No class completely failing (F1 >80%)
- ‚úÖ Balanced precision/recall

---

### [ ] Step 10: Save Final Model

```bash
# Your best model is at:
outputs/logs/<experiment_name>/checkpoints/best_model.pth

# Copy to safe location
cp outputs/logs/<experiment_name>/checkpoints/best_model.pth ./final_model.pth
```

---

## üêõ TROUBLESHOOTING

### Issue 1: CUDA Out of Memory

**Error**: `RuntimeError: CUDA out of memory`

**Solution**:
```yaml
# In configs/config.yaml
training:
  batch_size: 64  # Reduce from 128
  gradient_accumulation_steps: 2  # Maintain effective batch size
```

### Issue 2: Slow Training

**Symptoms**: <1000 images/second

**Checklist**:
1. ‚úÖ `compile_model.enabled: true`?
2. ‚úÖ `mixed_precision: true`?
3. ‚úÖ GPU utilization >90%?
4. ‚úÖ Using CUDA, not CPU?

**Debug**:
```python
python -c "import torch; print(torch.cuda.is_available())"
```

### Issue 3: Poor Validation Accuracy

**Symptoms**: Val accuracy stuck at 60-70%

**Solutions**:
1. Increase epochs (try 100)
2. Verify augmentation enabled
3. Check learning rate (should be 0.001-0.01)
4. Enable SAM for better generalization

### Issue 4: Training Crashes

**Error**: Various errors during training

**Solutions**:
```yaml
# Add gradient clipping
training:
  gradient_clip:
    enabled: true
    max_norm: 1.0
```

### Issue 5: Wandb Login Issues

**Error**: `wandb: ERROR`

**Solution**:
```bash
# Get API key from https://wandb.ai/settings
wandb login
# Paste your API key

# Or disable wandb
python train.py --epochs 50  # Without --wandb flag
```

---

## üìà EXPECTED TIMELINE

### Timeline for 50 Epochs (Recommended)

| Time | Event | Action |
|------|-------|--------|
| **0:00** | Start | Launch training |
| **0:05** | Epoch 1 | Verify running correctly |
| **0:30** | Epoch 2-3 | Check GPU usage |
| **2:00** | Epoch 10 | Check metrics trend |
| **4:00** | Epoch 20 | First checkpoint |
| **8:00** | Epoch 30 | Halfway point |
| **12:00** | Epoch 40 | SWA starts (epoch 38) |
| **16:00** | Epoch 48 | Almost done |
| **18:00** | Complete | Training finished ‚úÖ |
| **18:30** | Evaluate | Test set evaluation |

**Total**: ~20 hours from start to evaluation

---

## üéØ SUCCESS CRITERIA

### Training Success ‚úÖ
- [x] Training completes without crashes
- [x] Final train accuracy >90%
- [x] Final validation accuracy >88%
- [x] Loss converges (not oscillating)
- [x] GPU utilized efficiently (>90%)

### Evaluation Success ‚úÖ
- [x] Test accuracy: 93-95%
- [x] Generalization gap: <2%
- [x] All classes F1 >80%
- [x] Balanced precision/recall
- [x] Confusion matrix looks good

---

## üí° PRO TIPS

### Tip 1: Use Screen/Tmux for Long Training

```bash
# Install screen
sudo apt install screen

# Start screen session
screen -S training

# Run training
python train.py --epochs 50 --wandb

# Detach: Ctrl+A, then D
# Reattach: screen -r training
```

### Tip 2: Auto-Save Every N Epochs

Already configured in `config.yaml`:
```yaml
checkpoint:
  save_top_k: 3  # Keep best 3 models
  save_last: true  # Always save latest
```

### Tip 3: Resume from Crash

```bash
# Find latest checkpoint
ls outputs/logs/<experiment>/checkpoints/

# Resume training
python train.py --resume outputs/logs/<experiment>/checkpoints/checkpoint_epoch_30.pth --epochs 100
```

### Tip 4: Compare Multiple Runs

```bash
# Run 1: Baseline
python train.py --epochs 50 --wandb

# Run 2: No SAM
# Edit config: sam.enabled: false
python train.py --epochs 50 --wandb

# Compare on W&B dashboard
```

### Tip 5: Export Best Hyperparameters

After successful training:
```bash
# Save config used
cp configs/config.yaml outputs/logs/<experiment>/config_used.yaml
```

---

## üìû NEED HELP?

### Quick References
1. **README.md** - Project overview
2. **QUICKSTART.md** - Getting started
3. **PROJECT_ANALYSIS.md** - Full project analysis
4. **ADVANCED_TECHNIQUES.md** - Technical details

### Common Questions

**Q: How long will training take?**
A: 18-20 hours for 50 epochs on RTX 5090

**Q: Can I stop and resume?**
A: Yes! Use `--resume checkpoint.pth`

**Q: What if I get OOM errors?**
A: Reduce batch_size to 64 or 32

**Q: Should I use W&B?**
A: Yes! Much better visualization than TensorBoard

**Q: Can I train on CPU?**
A: Not recommended - will take 10x longer

---

## ‚ú® FINAL CHECKLIST

### Before Starting Training
- [x] Environment verified (`test_setup.py` passes)
- [x] Data present (`analyze_data.py` shows correct counts)
- [x] Config reviewed (`configs/config.yaml`)
- [x] GPU available (CUDA detected)
- [x] Enough disk space (>50 GB free)

### During Training
- [x] Training running without errors
- [x] Loss decreasing
- [x] Metrics improving
- [x] GPU utilized (>90%)
- [x] Checkpoints saving

### After Training
- [x] Best model saved
- [x] Evaluation completed
- [x] Results documented
- [x] Metrics meet targets (93-95%)

---

## üöÄ READY? LET'S GO!

```bash
# Navigate to project
cd /home/abnormal/Group34/Abnormal-Event-Detection-Model-8

# Final verification
python test_setup.py

# Start training (recommended)
python train.py --epochs 50 --wandb

# Watch it train! üéâ
```

---

**Good luck! You've got an excellent project - now train it and see the results! üöÄ**

**Estimated completion**: ~20 hours from now  
**Expected accuracy**: 93-95% on test set  
**Confidence**: Very high (excellent architecture + techniques)
